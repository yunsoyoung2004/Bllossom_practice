{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn28qgmjmpwU"
   },
   "source": [
    "# Bllossom-8B를 이용한 한국어 LLM 튜토리얼\n",
    "\n",
    "> [유튜브 빵형의 개발도상국](https://www.youtube.com/@bbanghyong)\n",
    "\n",
    "https://huggingface.co/MLP-KTLim/llama3-Bllossom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwtKm4_Wmud6"
   },
   "source": [
    "## 01. 활용할 package 설정\n",
    " - GPU사용하기: colab에서 런타임 --> 런타임유형변경 --> T4 선택\n",
    " - 패키지설치: 아래 pip를 이용해 Transformers, accelerate 설치\n",
    " - 런타임재시작: 런타임 --> 세션다시시작  (accelerate설치 시 런타임 다시시작하셔야됩니다!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5xmsZ2fOdAk"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.40.0 accelerate gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irNZYVFGnZEs"
   },
   "source": [
    "## 02. 모델준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655,
     "referenced_widgets": [
      "f5eee7a2044b4de88312915f66f416a5",
      "fc91fb4dee2341e49a1ed3e31aa698e6",
      "015efbb0de5949e9b79c02b5a7d041c6",
      "a00166b07ee247e0826a63c4be914ab9",
      "e80b1d645ac04f969a14054842d3a510",
      "50b4920e6d3b4a5c8dc16943cb3ef040",
      "52e71e34040a428180b64c0221b8f5ab",
      "da2c1fe0b8cd4d1faf86e1be061144a3",
      "52e5007715784bca877c8b36014b2f11",
      "a640262a6ceb40918dcbdbbc98a3994f",
      "3311eb140f1644779fc4b7bdc5278638"
     ]
    },
    "id": "rmF8wfz3NhZt",
    "outputId": "086e71e5-fa05-45a9-ff4a-33eb32a377d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5eee7a2044b4de88312915f66f416a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'MLP-KTLim/llama3-Bllossom'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHcxdlTTnhqs"
   },
   "source": [
    "## 03. 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlSSOAUuOUmh",
    "outputId": "8861f642-778a-452a-a3af-e989fccfd2c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최근 언어학계에서는 기계번역, 언어 이해 등 다양한 언어 처리 분야에서 활용되는 대규모 언어 모델의 한계를 극복하기 위해 다국어 데이터와 기술을 추가하는 언어 증강(Language Augmentation)이라는 개념이 주목받고 있습니다. 그러나 언어 모델은 언어별로 특징이 있어 단일 모델 내에서 서로 상충할 수 있으며, 이에 따라 언어 모델을 다언어 모델(Multilingual Model)로 전환하는 것은 쉽지 않습니다. 또한, 한국어와 같은 일부 언어들은 상대적으로 적은 양의 교육용 텍스트와 함께 자체적으로 학습 가능한 데이터가 부족하여 더욱 어려움이 따릅니다. 이에 연구자들은 언어 모델을 다언어 모델로 전환하면서도 각 언어의 독창성을 유지하기 위한 언어 증강 방법을 제안하였습니다. 이러한 연구 결과는 언어 모델이 다수 언어를 지원하더라도 각각의 언어의 독창성과 우수성을 보존할 필요가 있다는 것을 보여주었습니다. \n",
      "\n",
      "이러한 연구 결과를 바탕으로 한 논문'Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean\n"
     ]
    }
   ],
   "source": [
    "PROMPT = '''당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.'''\n",
    "instruction = \"다음 제목의 논문을 요약해줘 'Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean'\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q48YjmG6pfTG"
   },
   "source": [
    "## 04. 챗봇 인터페이스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "meGQ7F1bsZpy",
    "outputId": "0d31d422-1029-4231-8ff6-520f59991f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://57b4de1baffa01f4bc.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://57b4de1baffa01f4bc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# PROMPT = '''당신은 20대 초반의 여성이고 이름은 '빵순이'입니다. 귀여운 말투로 메신저로 대화하듯이 간결하고 정확하게 대답해야 합니다.'''\n",
    "PROMPT = '''당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.'''\n",
    "\n",
    "def generate_response(input_text, chat_history):\n",
    "    messages = [{\"role\": \"system\", \"content\": f\"{PROMPT}\"}]\n",
    "\n",
    "    for message in chat_history:\n",
    "        messages.append({\"role\": \"user\", \"content\": message[0]})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": message[1]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    chat_history.append((input_text, response))\n",
    "\n",
    "    return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=600)\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    msg.submit(generate_response, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbT1xb6OvRA-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
