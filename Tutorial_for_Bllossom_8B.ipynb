{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn28qgmjmpwU"
   },
   "source": [
    "# Bllossom-8B를 이용한 한국어 LLM 튜토리얼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwtKm4_Wmud6"
   },
   "source": [
    "## 01. 활용할 package 설정\n",
    " - GPU사용하기: colab에서 런타임 --> 런타임유형변경 --> T4 선택\n",
    " - 패키지설치: 아래 pip를 이용해 Transformers, accelerate 설치\n",
    " - 런타임재시작: 런타임 --> 세션다시시작  (accelerate설치 시 런타임 다시시작하셔야됩니다!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5xmsZ2fOdAk",
    "outputId": "296c5b87-b5f5-40d9-d53c-5c9f3d7fad8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.0\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/137.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.23.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (4.66.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.42.4\n",
      "    Uninstalling transformers-4.42.4:\n",
      "      Successfully uninstalled transformers-4.42.4\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 transformers-4.40.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.40.0 accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irNZYVFGnZEs"
   },
   "source": [
    "## 02. 모델준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "cb9538d004434558aa210b15138a4d04",
      "0409c37282c248fc9d91eb7eec7e4500",
      "5c11bcceea354d7ca1d4da1b8db24f8b",
      "ac5630f683934a01914277d476eb7e80",
      "9c0b2d47da5b495ca908fd2f67cd0b59",
      "1ab0be51a192497b84e6db1610e7139f",
      "67b0c540264d42b49d256846101981ab",
      "bc317c20704147e6b06860ae118dcaa5",
      "e82e1635220645ffb7d4f3b2e95ba2de",
      "2a601c19f164448c9832137f51ccfb02",
      "3fd5c262c79a4dd7b786b44366a7d933",
      "cd09723cc90a408eb4b1cbd9559c6af3",
      "08d429ffbf794a2c9c65010673b71b12",
      "b822b125d041489d9f54391641ff23a2",
      "d20e1d41f9bb4f87b17d009fd8ee1daa",
      "d06f78936be648e9827192365dc2b3fb",
      "23c4409010b345439198a32faec1d393",
      "6abdc63d056a4f4d98a8aea1553e7ae0",
      "8f51e011536e44e78ff848bc2ba6e482",
      "6ee2d702777841638be8a048f266c85f",
      "0c758280ca674f1281e8ae631ceb62df",
      "bd4038542c7d46308eacc9029e67e576",
      "949ee0f1ba6045ea99347a5c634de21c",
      "91d00ff1cd9f46f2850e18fd702f9150",
      "066a2faf1dbc4b5392813fd5b29ccbfd",
      "fb936abfd0e84f1b8031d6b96490a2f8",
      "3223e0d1f9f54871a513f9264b72d2f1",
      "eccd955174f246d19f3a6e701637877a",
      "773fdb3e7e244c3d8b50cd0337e528f0",
      "435ef86faf5f4115888a21d4f98b764f",
      "55847ba329a345bebeb0f08fde696753",
      "e42e81a31a2644e78400b4abc0c5384b",
      "59625c81319b4ae480e0edc46ecf76b2",
      "945704d3f08d4bb99bee69dcaefd9c85",
      "62b12dd2ad2a4239a4e46b2d33079bf3",
      "01f33ec8e3e74a48a1b3634ec6694338",
      "b1355ce262a54b2b9136379020c7c73d",
      "d555ab31e74e4953abdf4b8517d30bc1",
      "ad2eb2347c014798a9a0b53a497da0ed",
      "c52a0876ed0b49c89fd506376af867ba",
      "16e6b714b19d427b968f04d751797f9f",
      "927c799e4c3040c9be6c2fbcfc0f4066",
      "1dacafaefb8e462da0f48f9410a30764",
      "fd88eccf82144809b09917762b8f698b",
      "c718c693491741a1a9c5565feeed65aa",
      "0155aa7dadcc4c008bba5bea04910c53",
      "5cf5530aa85648c187aa2acd2680c344",
      "63a167061e594334b08c1eadb1624a6b",
      "b6840a6179e7431ca08acd0cb5b33b55",
      "4d3b83f13d41436b91d1329ed6f197a5",
      "af63ded0399b422993deccf5a4eae1ba",
      "212703a3331b49be8d99850ed46cf368",
      "adb118357d6f469c9cbfb049a73900e3",
      "41e80059b3af4d3991f1b790fa56fd04",
      "4baf8ebe44ff4bb58546c490680508cf",
      "e6970672f798468d97f173e3761ca791",
      "ba7ee050fc884cdcb3a54142ebcef8a0",
      "ed50a798c4f24d0f88953188e435d155",
      "78e07f646f0f4d2e91e877e943ebaacd",
      "3722e682e48448a6a912fa69223afef2",
      "7bcbeff0810e4dbf9abea2bbfd50dda4",
      "4ef554efbb964823807d69429ae0ed09",
      "a1a05aab9b6a44ba841261544848b9e5",
      "eebcf79e3a454c8e9e9ed5872f604896",
      "7eebd128ec3c4d549cd6141e7c8369da",
      "d80a5a82a05840f9b97a441e914b770c",
      "e149a34e71f2415a80fedb4ec0a6a258",
      "a3fa565d75bf4ff6a8d3e225c1c9c814",
      "24a5f9c58f544ebeaf18b2ba8ff75146",
      "c4a7e1b48f374aa9b8f54e5d38829db7",
      "fe52087dd76d41c6bde92bac40efdd8f",
      "783e9df8641f46f982a56de8e85f811b",
      "9a0cebc8f7b3450dbf519a0b32f27ed6",
      "3686ef99258f4ef59261bed6652e0b0a",
      "29290e1fafe44390a8bb9d76455c3aa6",
      "568d5aaf014f4012a4f8780a14cb4296",
      "e90160bd94b04a8fba03af282fb3ac4c",
      "568c12b5c3584262ad66c7673b0e32b8",
      "89a276e2c76149d898717d9d96bbf691",
      "2cb61cd5914c475cba710f8469ade178",
      "d3f40abb87f94d56997f6297a2fe0214",
      "7c6fb4aedd2c4f158641affe6ee66937",
      "0b9c79c4cc4a46ac95ae6ee7960d3dc6",
      "107b700dede34d4f8afd98c3e53a2550",
      "17c5f7931b6a42bcae0d4a911120b121",
      "8b147b286f7e4d03a6163ca245952647",
      "9eb31bf7cb544dc68de71d5e2203b546",
      "3145a273ead44b3f9a96918319d9a630",
      "c3948c0729a04f9fb8105fe009f67655",
      "eab8322bbd844c5fa68ed9de44485533",
      "919c943c5e724ef5ab80e709af4d3276",
      "2e636dc875894da68b9d6492f83400aa",
      "46cc2a81de8e4fa2a6652ef99b68f6cc",
      "ae5997af8d4244e4baf9cb0dc687fa05",
      "99cdf57cd2674ac19f5f9239d3972dd5",
      "fc5c7bf857fa4a288c82c34b337ff7f2",
      "5684b71f936f4ff582fbeb28224b5103",
      "5f69749b853a4561b964874423d4166b",
      "59f738e3667342bdb2f05e1130ae3c49",
      "16ef363512204401893d8abe9cd83400",
      "a71de1ff4dbd4586827de26970abcc2a",
      "a7e2103f1ea54bd38d89ba8bb24b4c3e",
      "648c1d5f0c484c7182b0cf8184d72301",
      "e510b919f436495792c9296b5b403c07",
      "1b751588827745bc9c14eedc60b69c6a",
      "30986403e33743808e990817f24be3f0",
      "0cdb80aa8c7f4730985cab6c3f38a8c8",
      "464b51f224df4635bea99bd15c5eb546",
      "2819ed6ba1514466a43700e93ec8903b",
      "152655c757f7490ca302767c5d49ceaa",
      "f9d137bd9f964898bbaeb19a152e5bc4",
      "0d2db753d3ee4df18752ddc294d7dfa3",
      "327216dc3c6645ceb31d58df8456163b",
      "ce674321fdf54ccfa3701104b16a7c60",
      "3edbe40edbab434b8a3a99102faa221b",
      "4783a112201d4dcda669b40cc4c694f8",
      "654845920cea413094892e62eda396f2",
      "bde10cca554a45a197d1100f08800526",
      "cdb06d54cfe64c2cb983726057284696",
      "e5677c1c938a42d782d3617920181063",
      "0143b39bec4f484eb00d2ef216e4f590",
      "9a3277f5b9214a83aa271b29456fb813",
      "49f3ab2ce1194c2698f6440dcde9e7ef",
      "6204d9eb9536402cb767c89b7c969a7a",
      "01c451227e9545b6b6eb1f55ad3bf8d5",
      "76bdca893be945508525fb006978064a",
      "e78c79b006ce47ac954ee8d1f8215a04",
      "33fd6059961345bc8640d7f4c56bdc99",
      "1186600aa69f476c900aab502e5ee096",
      "32f6fd0170b846ec983b651dc7554a4b",
      "afd5b3f5ddf84edb9a52b0fa420fe79a",
      "4b5105699c8e406694c9dfe1124ea224"
     ]
    },
    "id": "rmF8wfz3NhZt",
    "outputId": "69cacde5-0476-4f8e-81dd-3699fb615ad5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9538d004434558aa210b15138a4d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd09723cc90a408eb4b1cbd9559c6af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949ee0f1ba6045ea99347a5c634de21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945704d3f08d4bb99bee69dcaefd9c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c718c693491741a1a9c5565feeed65aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6970672f798468d97f173e3761ca791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e149a34e71f2415a80fedb4ec0a6a258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568c12b5c3584262ad66c7673b0e32b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3948c0729a04f9fb8105fe009f67655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ef363512204401893d8abe9cd83400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d137bd9f964898bbaeb19a152e5bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3277f5b9214a83aa271b29456fb813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHcxdlTTnhqs"
   },
   "source": [
    "## 03. 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlSSOAUuOUmh",
    "outputId": "d92b6b0d-0a8f-4128-abda-744d674ba51d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울과학기술대학교의 인공지능응용학과는 컴퓨터 과학, 수학, 통신 등 다양한 분야에서 인간의 지능을 모방하고 초월하는 기술을 연구하고 개발하기 위해 설립되었습니다. 이 학과는 학생들이 인공지능(AI), 머신러닝(ML), 딥러닝(DL) 등의 최신 기술을 배우고, 이를 실제 문제 해결에 적용할 수 있는 능력을 키울 수 있도록 교육합니다.\n",
      "\n",
      "### 학과의 주요 특징\n",
      "1. **전문 교수진**: 학과에는 AI 및 관련 분야에서 세계적으로 인정받는 전문가들이 있으며, 이들은 학생들에게 최신 연구 결과와 산업 현장에서의 경험을 공유합니다.\n",
      "2. **실습 중심의 교육**: 이론적 지식을 바탕으로 실습을 통해 학생들이 직접 프로젝트를 진행하며, 이를 통해 구체적인 문제 해결 능력과 창의력을 기를 수 있습니다.\n",
      "3. **산업 협력 프로그램**: 학과는 삼성, LG, SK 등 국내외 주요 기업들과의 협력을 통해 학생들이 산업 현장에서 필요한 기술과 경험이 쌓일 수 있도록 지원합니다.\n",
      "4. **연구 기회 제공**: 학생\n"
     ]
    }
   ],
   "source": [
    "PROMPT = '''You are a helpful AI assistant. Please answer the user's questions kindly. 당신은 유능한 AI 어시스턴트 입니다. 사용자의 질문에 대해 친절하게 답변해주세요.'''\n",
    "instruction = '''\n",
    "서울과학기술대학교의 인공지능응용학과에 대해 소개해줘\n",
    "\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    ]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty = 1.1\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QVN4Dolc_RY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
